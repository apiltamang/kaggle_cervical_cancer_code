{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from Executor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Vgg16 model loaded.\n",
      "Found 6166 images belonging to 3 classes.\n",
      "Found 1461 images belonging to 3 classes.\n",
      "initialized training data from: ../data/segmented/train\n",
      "initialized validation data from: ../data/segmented/valid\n",
      "found number of softmax classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Reset the model, and run same finetuning using precomputed conv. model outputs. See if they match...\n",
    "executor = ExecutorBuilder().\\\n",
    "    with_runID(\"augment\").\\\n",
    "    and_().\\\n",
    "    with_Vgg16().\\\n",
    "    and_().\\\n",
    "    train_batch_size(128). \\\n",
    "    and_(). \\\n",
    "    learn_rate(0.001).\\\n",
    "    and_().\\\n",
    "    data_on_path(\"../data/segmented/\").\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting rescaled fc model...\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "done...\n",
      "('updating dropout from: ', 0.5, ' to: ', 0.5)\n"
     ]
    }
   ],
   "source": [
    "fc_model = executor.get_rescaled_fc_model(new_dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load a finely tuned model\n",
    "fc_model.load_weights(\"fc_model_val_loss=0_71.segmented.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading precomputed conv. outputs...\n",
      "done...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Executor.Executor instance at 0x7f30f0a5e518>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the precomputed conv. model outputs\n",
    "executor.load_precomputed_conv_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Executor.Executor instance at 0x7f30f0a5e518>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.load_model_from_file(\"vgg16_with_dense_layer_trained_to_0_71.segmented.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting rescaled fc model...\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "done...\n",
      "('updating dropout from: ', 0.0, ' to: ', 0.0)\n"
     ]
    }
   ],
   "source": [
    "fc_model = executor.get_rescaled_fc_model(new_dropout=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model.save_weights(\"overfitted_fc_model.dropout_0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt=Adam(lr=0.000001)\n",
    "fc_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6166 samples, validate on 1461 samples\n",
      "Epoch 1/5\n",
      "6166/6166 [==============================] - 7s - loss: 0.1513 - acc: 0.9854 - val_loss: 0.7373 - val_acc: 0.6879\n",
      "Epoch 2/5\n",
      "6166/6166 [==============================] - 6s - loss: 0.1251 - acc: 0.9904 - val_loss: 0.7426 - val_acc: 0.6886\n",
      "Epoch 3/5\n",
      "6166/6166 [==============================] - 6s - loss: 0.1074 - acc: 0.9934 - val_loss: 0.7447 - val_acc: 0.6879\n",
      "Epoch 4/5\n",
      "6166/6166 [==============================] - 6s - loss: 0.0924 - acc: 0.9964 - val_loss: 0.7470 - val_acc: 0.6899\n",
      "Epoch 5/5\n",
      "6166/6166 [==============================] - 6s - loss: 0.0798 - acc: 0.9976 - val_loss: 0.7583 - val_acc: 0.6913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3027993350>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build extremely overfitted model with a 0.0 dropout\n",
    "fc_model.fit(executor.train_precomputed, executor.train_labels, nb_epoch=5,\n",
    "               batch_size= executor.batch_size, validation_data=(executor.val_precomputed, executor.val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rescale=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "conv_model = executor.conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in conv_model.layers:\n",
    "    layer.trainable = False\n",
    "for layer in fc_model.layers:\n",
    "    conv_model.add(layer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/segmented/train\n",
      "<keras.preprocessing.image.DirectoryIterator object at 0x7f30ecfa2e50>\n"
     ]
    }
   ],
   "source": [
    "print executor.train_path\n",
    "print executor.val_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6166 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "aug_batches = executor.vgg.get_batches(path=executor.train_path, gen=gen, batch_size=executor.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6166/6166 [==============================] - 199s - loss: 1.1554 - acc: 0.5527 - val_loss: 0.7583 - val_acc: 0.6913\n",
      "Epoch 2/5\n",
      "6166/6166 [==============================] - 199s - loss: 1.1622 - acc: 0.5551 - val_loss: 0.7436 - val_acc: 0.7036\n",
      "Epoch 3/5\n",
      "6166/6166 [==============================] - 199s - loss: 1.1603 - acc: 0.5435 - val_loss: 0.7427 - val_acc: 0.7009\n",
      "Epoch 4/5\n",
      "6166/6166 [==============================] - 199s - loss: 1.1602 - acc: 0.5517 - val_loss: 0.7561 - val_acc: 0.6899\n",
      "Epoch 5/5\n",
      "6166/6166 [==============================] - 199s - loss: 1.1661 - acc: 0.5493 - val_loss: 0.7591 - val_acc: 0.6920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f301acfbc10>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit_generator(aug_batches, samples_per_epoch=aug_batches.nb_sample, validation_data=executor.val_batches, \n",
    "                         nb_val_samples=executor.val_batches.nb_sample, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6166/6166 [==============================] - 199s - loss: 1.1468 - acc: 0.5553 - val_loss: 0.7562 - val_acc: 0.6934\n",
      "Epoch 2/5\n",
      "6166/6166 [==============================] - 198s - loss: 1.1387 - acc: 0.5560 - val_loss: 0.7583 - val_acc: 0.6913\n",
      "Epoch 3/5\n",
      "6166/6166 [==============================] - 198s - loss: 1.1689 - acc: 0.5467 - val_loss: 0.7555 - val_acc: 0.6899\n",
      "Epoch 4/5\n",
      "6166/6166 [==============================] - 198s - loss: 1.1422 - acc: 0.5511 - val_loss: 0.7641 - val_acc: 0.6940\n",
      "Epoch 5/5\n",
      "6166/6166 [==============================] - 199s - loss: 1.1749 - acc: 0.5376 - val_loss: 0.7757 - val_acc: 0.6817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3019188590>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt=Adam(lr=0.00001)\n",
    "conv_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])    \n",
    "conv_model.fit_generator(aug_batches, samples_per_epoch=aug_batches.nb_sample, validation_data=executor.val_batches, \n",
    "                         nb_val_samples=executor.val_batches.nb_sample, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.save_weights(\"conv_model.data_augmented.segmented.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Vgg16 model loaded.\n",
      "Found 6166 images belonging to 3 classes.\n",
      "Found 1461 images belonging to 3 classes.\n",
      "initialized training data from: ../data/segmented/train\n",
      "initialized validation data from: ../data/segmented/valid\n",
      "found number of softmax classes: 3\n"
     ]
    }
   ],
   "source": [
    "executor = ExecutorBuilder().\\\n",
    "    with_runID(\"augment\").\\\n",
    "    and_().\\\n",
    "    with_Vgg16().\\\n",
    "    and_().\\\n",
    "    train_batch_size(128). \\\n",
    "    and_(). \\\n",
    "    learn_rate(0.001).\\\n",
    "    and_().\\\n",
    "    data_on_path(\"../data/segmented/\").\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Executor.Executor instance at 0x7f30183650e0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.load_model_from_file(\"conv_model.data_augmented.segmented.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 512 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Executor.Executor instance at 0x7f30183650e0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.build_predictions_on_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='predictions.vgg16_data_aug.loss_77.h5' target='_blank'>predictions.vgg16_data_aug.loss_77.h5</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/kaggle_cervical_cancer_code/src/predictions.vgg16_data_aug.loss_77.h5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.save_predictions_to_file(\"predictions.vgg16_data_aug.loss_77.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
